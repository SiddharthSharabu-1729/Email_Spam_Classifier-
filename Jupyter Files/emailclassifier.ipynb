{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779a1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e939b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2d98d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "__author__ = \"Siddharth Achari Sharabu\"\n",
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a4c69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "%run Utilities.ipynb import get_model, SEQUENCE_LENGTH, TEST_SIZE\n",
    "%run Utilities.ipynb import BATCH_SIZE, EPOCHS, label2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7d02d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    texts, labels = [], []\n",
    "    with open(\"data/spam.txt\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            labels.append(split[0].strip())\n",
    "            texts.append(' '.join(split[1:]).strip())\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a494751",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d6bb07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 472, 4436, 843, 756, 659, 64, 8, 1328, 87, 123, 352, 1329, 148, 2996, 1330, 67, 58, 4437, 144]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDDHA~1\\AppData\\Local\\Temp/ipykernel_6540/3571508011.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0   49  472 4436  843\n",
      "  756  659   64    8 1328   87  123  352 1329  148 2996 1330   67   58\n",
      " 4437  144]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "pickle.dump(tokenizer, open(\"results/tokenizer.pickle\", \"wb\"))\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "print(X[0])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4c6ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "y = [ label2int[label] for label in y ]\n",
    "y = to_categorical(y)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9042798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e857bbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4180, 100)\n",
      "X_test.shape: (1394, 100)\n",
      "y_train.shape: (4180, 2)\n",
      "y_test.shape: (1394, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762626de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:22, 17660.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          901300    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,018,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 901,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(tokenizer=tokenizer, lstm_units=128)\n",
    "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}.h5\", save_best_only=True,verbose=1)\n",
    "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4f53a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9354 - precision: 0.9354 - recall: 0.9354\n",
      "Epoch 1: val_loss improved from inf to 0.27557, saving model to results\\spam_classifier_0.28.h5\n",
      "66/66 [==============================] - 36s 430ms/step - loss: 0.1683 - accuracy: 0.9354 - precision: 0.9354 - recall: 0.9354 - val_loss: 0.2756 - val_accuracy: 0.8802 - val_precision: 0.8802 - val_recall: 0.8802\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9699 - precision: 0.9699 - recall: 0.9699\n",
      "Epoch 2: val_loss improved from 0.27557 to 0.07908, saving model to results\\spam_classifier_0.08.h5\n",
      "66/66 [==============================] - 26s 399ms/step - loss: 0.0889 - accuracy: 0.9699 - precision: 0.9699 - recall: 0.9699 - val_loss: 0.0791 - val_accuracy: 0.9720 - val_precision: 0.9720 - val_recall: 0.9720\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9780 - precision: 0.9780 - recall: 0.9780\n",
      "Epoch 3: val_loss did not improve from 0.07908\n",
      "66/66 [==============================] - 25s 384ms/step - loss: 0.0692 - accuracy: 0.9780 - precision: 0.9780 - recall: 0.9780 - val_loss: 0.1048 - val_accuracy: 0.9620 - val_precision: 0.9620 - val_recall: 0.9620\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9811 - precision: 0.9811 - recall: 0.9811\n",
      "Epoch 4: val_loss improved from 0.07908 to 0.06171, saving model to results\\spam_classifier_0.06.h5\n",
      "66/66 [==============================] - 27s 415ms/step - loss: 0.0606 - accuracy: 0.9811 - precision: 0.9811 - recall: 0.9811 - val_loss: 0.0617 - val_accuracy: 0.9799 - val_precision: 0.9799 - val_recall: 0.9799\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9840 - precision: 0.9840 - recall: 0.9840\n",
      "Epoch 5: val_loss did not improve from 0.06171\n",
      "66/66 [==============================] - 26s 389ms/step - loss: 0.0507 - accuracy: 0.9840 - precision: 0.9840 - recall: 0.9840 - val_loss: 0.0704 - val_accuracy: 0.9749 - val_precision: 0.9749 - val_recall: 0.9749\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9878 - precision: 0.9878 - recall: 0.9878\n",
      "Epoch 6: val_loss improved from 0.06171 to 0.05976, saving model to results\\spam_classifier_0.06.h5\n",
      "66/66 [==============================] - 26s 387ms/step - loss: 0.0397 - accuracy: 0.9878 - precision: 0.9878 - recall: 0.9878 - val_loss: 0.0598 - val_accuracy: 0.9813 - val_precision: 0.9813 - val_recall: 0.9813\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9892 - precision: 0.9892 - recall: 0.9892\n",
      "Epoch 7: val_loss did not improve from 0.05976\n",
      "66/66 [==============================] - 25s 385ms/step - loss: 0.0357 - accuracy: 0.9892 - precision: 0.9892 - recall: 0.9892 - val_loss: 0.0622 - val_accuracy: 0.9821 - val_precision: 0.9821 - val_recall: 0.9821\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9926 - precision: 0.9926 - recall: 0.9926\n",
      "Epoch 8: val_loss did not improve from 0.05976\n",
      "66/66 [==============================] - 26s 387ms/step - loss: 0.0291 - accuracy: 0.9926 - precision: 0.9926 - recall: 0.9926 - val_loss: 0.0639 - val_accuracy: 0.9770 - val_precision: 0.9770 - val_recall: 0.9770\n",
      "Epoch 9/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9923 - precision: 0.9923 - recall: 0.9923\n",
      "Epoch 9: val_loss improved from 0.05976 to 0.05311, saving model to results\\spam_classifier_0.05.h5\n",
      "66/66 [==============================] - 25s 386ms/step - loss: 0.0270 - accuracy: 0.9923 - precision: 0.9923 - recall: 0.9923 - val_loss: 0.0531 - val_accuracy: 0.9857 - val_precision: 0.9857 - val_recall: 0.9857\n",
      "Epoch 10/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952\n",
      "Epoch 10: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 389ms/step - loss: 0.0219 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952 - val_loss: 0.0715 - val_accuracy: 0.9821 - val_precision: 0.9821 - val_recall: 0.9821\n",
      "Epoch 11/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9931 - precision: 0.9931 - recall: 0.9931\n",
      "Epoch 11: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 386ms/step - loss: 0.0211 - accuracy: 0.9931 - precision: 0.9931 - recall: 0.9931 - val_loss: 0.0581 - val_accuracy: 0.9828 - val_precision: 0.9828 - val_recall: 0.9828\n",
      "Epoch 12/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9943 - precision: 0.9943 - recall: 0.9943\n",
      "Epoch 12: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 396ms/step - loss: 0.0184 - accuracy: 0.9943 - precision: 0.9943 - recall: 0.9943 - val_loss: 0.0608 - val_accuracy: 0.9799 - val_precision: 0.9799 - val_recall: 0.9799\n",
      "Epoch 13/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952\n",
      "Epoch 13: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 393ms/step - loss: 0.0157 - accuracy: 0.9952 - precision: 0.9952 - recall: 0.9952 - val_loss: 0.1009 - val_accuracy: 0.9778 - val_precision: 0.9778 - val_recall: 0.9778\n",
      "Epoch 14/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9964 - precision: 0.9964 - recall: 0.9964\n",
      "Epoch 14: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 389ms/step - loss: 0.0118 - accuracy: 0.9964 - precision: 0.9964 - recall: 0.9964 - val_loss: 0.0878 - val_accuracy: 0.9864 - val_precision: 0.9864 - val_recall: 0.9864\n",
      "Epoch 15/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9967\n",
      "Epoch 15: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 25s 385ms/step - loss: 0.0125 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9967 - val_loss: 0.0727 - val_accuracy: 0.9813 - val_precision: 0.9813 - val_recall: 0.9813\n",
      "Epoch 16/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9957 - precision: 0.9957 - recall: 0.9957\n",
      "Epoch 16: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 388ms/step - loss: 0.0111 - accuracy: 0.9957 - precision: 0.9957 - recall: 0.9957 - val_loss: 0.0777 - val_accuracy: 0.9813 - val_precision: 0.9813 - val_recall: 0.9813\n",
      "Epoch 17/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9981 - precision: 0.9981 - recall: 0.9981\n",
      "Epoch 17: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 25s 385ms/step - loss: 0.0071 - accuracy: 0.9981 - precision: 0.9981 - recall: 0.9981 - val_loss: 0.0991 - val_accuracy: 0.9842 - val_precision: 0.9842 - val_recall: 0.9842\n",
      "Epoch 18/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9976 - precision: 0.9976 - recall: 0.9976\n",
      "Epoch 18: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 25s 386ms/step - loss: 0.0081 - accuracy: 0.9976 - precision: 0.9976 - recall: 0.9976 - val_loss: 0.0794 - val_accuracy: 0.9828 - val_precision: 0.9828 - val_recall: 0.9828\n",
      "Epoch 19/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988 - precision: 0.9988 - recall: 0.9988\n",
      "Epoch 19: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 27s 403ms/step - loss: 0.0041 - accuracy: 0.9988 - precision: 0.9988 - recall: 0.9988 - val_loss: 0.0751 - val_accuracy: 0.9857 - val_precision: 0.9857 - val_recall: 0.9857\n",
      "Epoch 20/20\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9993 - precision: 0.9993 - recall: 0.9993\n",
      "Epoch 20: val_loss did not improve from 0.05311\n",
      "66/66 [==============================] - 26s 395ms/step - loss: 0.0035 - accuracy: 0.9993 - precision: 0.9993 - recall: 0.9993 - val_loss: 0.0893 - val_accuracy: 0.9849 - val_precision: 0.9849 - val_recall: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190dc630e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          callbacks=[tensorboard, model_checkpoint],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da5ce82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 2s 36ms/step - loss: 0.0893 - accuracy: 0.9849 - precision: 0.9849 - recall: 0.9849\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf5a7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "precision = result[2]\n",
    "recall = result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "598da55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Accuracy: 98.49%\n",
      "[+] Precision:   98.49%\n",
      "[+] Recall:   98.49%\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
    "print(f\"[+] Recall:   {recall*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
